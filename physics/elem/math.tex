\chapter{Vectors}
\section{Vectors}

Vectors are quantities with magnitude and direction, contrastingly to scalars which only have magnitude. 
For our purposes, vectors can be understood as directed line segments, although
they're more rigorously defined as elements of a vector space, or can be 
understood through rotations. We'll talk about rotations later, but will not deal with
vector spaces right now.

I shall use boldface to denote vectors, such as \(\vec{A}\). 

The utility of vectors stem from their independence from a particular co-ordinate system. They're
directed \emph{line segments}, only their length
matters and we're not concerned with their end points. This means that a vector 
can be freely translated. 

\begin{marginfigure}
    \scalebox{2}{\incfig{transvectors}}
    \caption{Identical Vectors}
\end{marginfigure}

The magnitude of a vector, its norm, is its euclidean ``length'', we denote it
by \(\norm*{\vec{A}}\) or simply \(A\). 

A unit vector, \(\unitv{a}\) (``a hat'') is a vector whose magnitude is unity.
We use the unit vector to often denote the direction of a vector by multiplying 
the unit vector with its magnitude. For instance, the unit vector
that point is the direction of \(A\), \(\unitv{A}\) can be calculated as,
\[ \unitv{A} = \frac{\vec{A}}{A}\] 
where we divide by the magnitude of \(A\) to ensure that the magnitude of \(\unitv{a}\) is
\(1\).

\section{Vector Algebra}    

There are two operations generally defined on vectors, \vocab{scalar multiplication}
and \vocab{vector addition}. 

\subsection{Scalar Multiplication}

Multiplying a vector \(\vec{A}\) by a scalar \(c\) results
in another vector, \(c\vec{A}\) parallel or antiparallel(pointing in the opposite 
direction) to \(\vec{A}\) scaled by a factor of \(\abs*{c}\).

\newpage

\begin{marginfigure}
    \hspace{-5em}
    \vspace{-2em}
    \centering
    \scalebox{2.5}{\incfig{scalarmultiplication}}
    \caption{Scalar Multiplication. The vector, \(\vec{A}\) multiplied 
    by a scalar, \(c\) where \(c > 1\).}
\end{marginfigure}


If \(c\) is positive, then the resultant vector is simply scaled by it.
If we multiply a vector by \(-1\), then the result is the simple
reversal in direction of vectors. If \(c\) is negative, then the 
vector is reversed and scaled by \(\abs*{c}\). 

\subsection{Vector Addition}

We define an operation called addition of vectors which produces another vector.
To add two vectors, \(\vec{A}\) and \(\vec{B}\), we stick the tail
of one vector with the head of another. 

We can calculate the magnitude of their resultant vector, \(\norm*{\vec{A} + \vec{B}}\)
by using the cosine rule. In \cref{fig: vectoradd} \(\theta\) is the angle between 
\(\vec{A}\) and \(\vec{B}\).

\begin{align*}
    &\norm*{\vec{A} + \vec{B}}^2 = A^2 + B^2 - 2AB\cos(\pi - \theta) \\
    &\norm*{\vec{A} + \vec{B}}^2 = A^2 + B^2 + 2AB\cos\theta \\
    &\norm*{\vec{A} + \vec{B}} = \sqrt{A^2 + B^2 + 2AB\cos\theta}
\end{align*}

To subtract two vectors, we simply reverse the vector and then add the resulting vector.

\begin{marginfigure}
    \scalebox{2}{\incfig{vectoradd}}
    \caption{Vector Addition}
    \label{fig: vectoradd}
\end{marginfigure}

\begin{example}
    Consider the infinitesimal change vector, \(\dd{\vec{A}} = \vec{A}(t + \dd{t}) - \vec{A}(t)\).
    
    We have, 
    \begin{align*}
        \dd{\vec{A}} &= A(t + \dd{t})\unitv{A}(t + \dd{t}) - A(t)\unitv{A}(t) \\
        \dd{\vec{A}} &= A(t + \dd{t})\unitv{A}(t + \dd{t}) - A(t + \dd{t})\unitv{A}(t) \\ 
        & + A(t + \dd{t})\unitv{A}(t) - A(t)\unitv{A}(t) \\
        \dd{\vec{A}} &= A(t + \dd{t})(\unitv{A}(t + \dd{t}) - \unitv{A}(t)) + \unitv{A}(t)(A(t + \dd{t}) - A(t)) \\
        \dd{\vec{A}} &= A\dd{\unitv{A}} + \dd{A}\unitv{A} 
    \end{align*}
The first term represents a change in direction and the second represents a change in 
magnitude.
\end{example}

The magnitude of the first can be calculated as,

First consider \cref{fig: timedev}.

Now, let us first work out a geometric argument for \(A\dd{\unitv{A}}\)
which is perpendicular to \(\vec{A}\), we can show its perpendicular because 
the other term, \(\dd{A} \unitv{A}\) is parallel to \(A\), and causes no change in direction. 

The second term, thus must account for change in direction. We can also see that \(A\dd{\unitv{A}}\)
causes no reasonable change in the magnitude of \(\vec{A}\). Therefore, it must be perpendicular to \(\vec{A}\)\footnote{This is because we are basically stating that \(\dd{\vec{A}}\) can be broken into two parts,
one parallel to \(\vec{A}\) and one perpendicular to it. The perpendicular one causes 
no change in magnitude.}.

For now, let us consider \(\increment \vec{A}\) perpendicular to \(\vec{A}\) as an approximation. We see that 

\begin{equation*}
    A(t_2) = \sqrt{A^2(t_1) + (\increment A)^2}
\end{equation*}

Therefore, for small \(\increment A\), the change is very small (it vanishes, essentially 
in the limiting case, \((dx)^2 = 0\) since it is a second order differential).

This allows us to consider a simple geometric argument which gives \(\norm{\increment \vec{A}} = 2A\sin(\theta/2)\)
where \(\theta\) is the angle between \(\vec{A}(t_1)\) and \(\vec{A}(t_2)\).

Since we're considering values of small \(\theta\), we may use the small angle approximation,
\(\sin(\theta) \approx \theta\). Therefore,

\begin{marginfigure}
    \centering
    \scalebox{2}{\incfig{timedev}}
    \caption{\(\increment\vec{A} \perp \vec{A}\)}
    \label{fig: timedev}
\end{marginfigure}

\begin{align}
    \norm{\vec{\increment A}} &\approx 2A \times \frac{\theta}{2}\\ 
    &\approx A \times \theta
\end{align}

Which is equivalent to \(A \increment \theta\). In the limit \(\increment \theta \to 0\),

\begin{equation}
    \dd{A_{\perp}} = A \dd{\theta}
\end{equation}

Since \(\dd{A}_{\perp} = A\norm*{\dd{\unitv{A}}}\),

\begin{equation}
    \norm*{\dd{\unitv{A}}} = \dd{\theta}
\end{equation}

\section{Dot Product}

The \vocab{dot product} of two vectors results in a scalar and is defined as,
\[\vec{A} \dotproduct \vec{B} = AB \cos\theta\]
where \(\theta\) is the angle between \(\vec{A}\) and \(\vec{B}\). 

Note that the quantity \(\vec{A} \dtp \vec{B}\) is just \(A\) times 
the magnitude of the projection of \(\vec{B}\) on \(\vec{A}\). Similarly, \(\vec{B} \dtp \vec{A}\) is
magnitude of the projection of \(\vec{A}\) on \(\vec{B}\) times \(B\).

We may note that,

\[\vec{A} \dtp \vec{A} = A^2\] Or,
\[A = \sqrt{\vec{A} \dtp \vec{A}}\]

It can be easily shown that the scalar product distributes over vector addition. 
Another thing of importance is the identity(which is also easy to derive),

\[\dv{\vec{A} \dtp \vec{B}}{t} = \dv{\vec{A}}{t} \dtp \vec{B} + \dv{\vec{B}}{t} \dtp \vec{A}\]

\section{Cross Product}

The \vocab{cross product} is another product operation on vectors, this product produces a vector.

It is defined as, 
\[\vec{A} \cp \vec{B} = AB\sin\theta \unitv{n}\] 

Where \(\unitv{n}\) is the vector perpendicular to the plane containing \(\vec{A}\) and \(\vec{B}\).
Since there are two directions that \(\unitv{n}\) can have, we define the two vectors 
and their cross product to form a right-hand triple.

Place your fingers in the direction of \(\vec{A}\) and curl them, along the smaller angle, towards 
\(\vec{B}\). Then the direction in which your thumb is pointing is the direction of \(\vec{A} \cp \vec{B}\). 

\section{Vectors in Component form}

Although we have worked without co-ordinate systems till now,
to actually extract any meaning from vector operations, a coordinate system is necessary.

A co-ordinate system consists of three things --- an origin, some axes and basis vectors.
The concept of an origin and axes is already familiar to us, for instance, in the form of the cartesian plane.
So our talk will be mostly centered around basis vectors. 

Suppose we have a vector \(\vec{C}\) which is the sum of two vectors \(\alpha\vec{A}\) and 
\(\beta\vec{B}\). We can then write \(\vec{C}\) as,

\begin{equation*}
    \vec{C} = \alpha\vec{A} + \beta\vec{B}
\end{equation*}

We call \(\vec{C}\) a linear combination of \(\vec{A}\) and \(\vec{B}\) and in essence, 
we can represent \(\vec{C}\) in terms of \(\vec{A}\) and \(\vec{B}\). 

The idea of basis vectors is based around this --- we adopt a set of vectors in terms 
of which we can represent every other vector. Note, of course, that we have 
a bit of a problem here. We can't really adopt vectors that are linear combinations of 
others --- otherwise we could simply replace them by their linear combination.

So basis vectors are vectors that are linearly independent. That is,
their linear combination when equated to the \(n\) dimensional zero vector, 
\[
    c_1\vec{e}_1 + c_2\vec{e}_2 + \dots + c_n\vec{e}_n = \vec{0}\] 

only has a trivial solution of \(c_1 = c_2 = \dots = c_n = 0\). Otherwise, we could have simply written the term \(\vec{e}_i\) as the linear combination of other vectors whenever we needed to use it.

Basis vectors which are orthogonal --- mutually perpendicular, form an \vocab{orthonormal} basis.

Although it might not be so obvious, all orthogonal sets of vectors are linearly 
independent. If we represent these as \(\Set{\vec{e}_1, \vec{e}_2, \dots, \vec{e}_n}\), we
can construct a set of basis vectors that are not orthogonal as,
\(\Set{\vec{e}_1, \vec{e}_2, \dots, \vec{e}_{n-1}, \vec{e}_1 + \vec{e_2} + \dots + \vec{e}_n}\).

Which means that we don't always need the basis vectors to be orthogonal, but
in practice we usually adopt an orthonormal basis.

In particular, we'll use the basis vectors which have magnitude of \(1\), i.e. unit vectors.

In cartesian co-ordinates, or when using the orthonormal cartesian basis, we use the unit vectors along the three axes, \(x\), \(y\) and 
\(z\). These are written as \(\unitv{x}\), \(\unitv{y}\) and \(\unitv{z}\) respectively.

But how can we write an arbitrary vector \(\vec{A}\) in terms of these unit vectors?
The answer lies in projecting the vector along the axes. We use the dot product for it.

\begin{align}
    A_x &= \vec{A} \dtp \unitv{x} = A \cos \alpha \\
    A_y &= \vec{A} \dtp \unitv{y} = A \cos \beta \\
    A_z &= \vec{A} \dtp \unitv{z} = A \cos \gamma 
\end{align}

Where \(\alpha\), \(\beta\), and \(\gamma\) are the angles 
the vector makes with the \(x\), \(y\) and \(z\) axes respectively.
The cosine of these angles are called \vocab{directional cosines}. 

Thus, we write \(\vec{A}\) in terms of it elements, in matrix form, as, 

\begin{equation}
    \vec{A} = 
    \begin{pmatrix}
        A_x \\
        A_y \\
        A_z
    \end{pmatrix}
\end{equation}
    

Now scalar multiplication is simply, 

\[
    c\vec{A} = 
    \begin{pmatrix}
        cA_x \\
        cA_y \\
        cA_z
    \end{pmatrix}
\]

And vector addition is,

\[
    \vec{A} + \vec{B} = 
    \begin{pmatrix}
        A_x + B_x\\
        A_y + B_y\\
        A_z + B_z 
    \end{pmatrix}
\]

For the dot product, representing \(\vec{A}\) and \(\vec{B}\) in terms of the basis vectors,

\begin{align*}
    \vec{A} \dtp \vec{B} &= (A_x \unitv{x} + A_y \unitv{y} + A_z \unitv{z}) \dtp (B_x \unitv{x} + B_y \unitv{y} + B_z \unitv{z}) \\
    \vec{A} \dtp \vec{B} &= A_xB_x + A_yB_y + A_zB_z
\end{align*}

Where we use the distributive property of dot product and the fact 
that \(\unitv{x} \dtp \unitv{y} = \unitv{y} \dtp \unitv{z} = \unitv{z} \dtp \unitv{x} = 0\).

We can find the angle between two vectors, \(\theta\), as,

\begin{marginfigure}
    \centering
    \scalebox{1.4}{\incfig{vectproject1}}
    \caption{Two arbitrary vectors}
\end{marginfigure}

\begin{equation*}
    \cos\theta = \frac{\vec{A} \dtp \vec{B}}{AB}
\end{equation*}

Note that now we can calculate the magnitude of \(\vec{A}\) as, 

\begin{equation*}
    \vec{A} \dtp \vec{A} = A^2 = A_x^2 + A_y^2 + A_z^2
\end{equation*}

which gives the magnitude as \(A = \sqrt{A_x^2 + A_y^2 + A_z^2}\).

Also, a very neat observation from the dot product is that,
\begin{equation*}
    \vec{A} \dtp \vec{B} \le AB
\end{equation*}

This is a rather simple idea since \(\cos\theta \in [-1, 1]\). 

Now if generalize this to \(n\) dimensions, we will find the inner product,
\(\vec{A} \dtp {B} = AB \cos\theta\) with each 
vector lying in an \(n\) dimensional space with \(n\) axis components. This has similar properties to that of the dot 
product and has the same but extended computation for 
the component form. Let \((a_1, a_2, \dots, a_n)\) be the components of \(\vec{A}\) and 
\((b_1, b_2, \dots, b_n)\) be the components of \(\vec{B}\), then,
\begin{equation*}
    \vec{A} \dtp \vec{B} \le AB
\end{equation*}

Using the magnitude formula we derived earlier and that the inner product has the same 
computation(sum of the product of same axis components),
\begin{equation}
    a_1b_1 + a_2b_2 + \dots + a_nb_n \le \sqrt{a_1^2 + a_2^2 + \dots + a_n^2}\sqrt{b_1^2 + b_2^2 + \dots b_n^2}
\end{equation}

which is the \emph{Cauchy-Shwartz} inequality. Note that the equality is when \(\cos\theta=0\).
Thus, when \(\vec{A}\) lies parallel to \(\vec{B}\). This can also be interpreted as \(\vec{A}\) being 
being a scaling of \(\vec{B}\) by some real number \(\lambda = B/A\). 

Given two vectors \(\vec{A}\) and \(\vec{B}\), we can find the projection of one along the \emph{direction} of 
the other by using the dot product.

\begin{marginfigure}
    \centering
    \scalebox{2.3}{\incfig{vectproject2}}
    \caption{\(\vec{A}\) projected along directions tangential and 
    perpendicular to \(\vec{B}\)}
    \label{fig: vectproject}
\end{marginfigure}

We use the fact that the vector \(\vec{A}\) along \(\vec{B}\) is simply 
\(\vec{A} \dtp \unitv{B}\). Since this is the magnitude, we now simply 
multiply this by \(\unitv{B}\). 

The projection of \(\vec{A}\) along the direction perpendicular
to \(\vec{B}\) is simply \(\vec{A} - (\vec{A} \dtp \unitv{B})\unitv{B}\). 

The two directions can be seen in \cref{fig: vectproject}.

\begin{example}
    Consider the unit vectors along the incident, reflected, and normal ray.
    Given that the unit vectors along the incident ray and normal ray are 
    \(\unitv{I}\) and \(\unitv{N}\) respectively, find the reflected unit vector.
    
    \begin{soln}
        The projection of \(\unitv{I}\) perpendicular and tangential to the 
        normal are \(\unitv{I} - (\unitv{I} \dtp \unitv{N})\unitv{N}\) and \((\unitv{I} \dtp \unitv{N})\unitv{N}\).

        Clearly, for the reflected unit vector, the direction of the perpendicular will remain the same 
        while of the tangential will be flipped. Thus, the components of 
        \(\unitv{R}\) along the normal are, \(\unitv{I} - (\unitv{I} \dtp \unitv{N})\unitv{N}\)
        and \(-(\unitv{I} \dtp \unitv{N})\unitv{N}\). Adding these vectors gives us \(\unitv{R}\),

        \begin{equation*}
            \unitv{R} = \unitv{I} - 2(\unitv{I} \dtp \unitv{N})\unitv{N}
        \end{equation*}
    \end{soln}
\end{example}

\begin{marginfigure}
    \centering
    \scalebox{1.6}{\incfig{reflectionvect}}
    \caption{Incidence, normal and reflected unit vectors.}
\end{marginfigure}


Finally, the cross product is a little difficult to compute.
It can be computed either as the determinant,

\[\vec{A} \cp \vec{B} =
\begin{vmatrix}
    \unitv{x} & \unitv{y} & \unitv{z} \\
    A_x & A_y & A_z \\
    B_x & B_y & B_z
\end{vmatrix}\]

Or using the matrix notation,
\[
\begin{pmatrix}
    A_x \\
    A_y \\
    A_z
\end{pmatrix} \cp
%
\begin{pmatrix}
    B_x \\
    B_y \\
    B_z
\end{pmatrix}
%
=
%
\begin{pmatrix}
    A_yB_z - A_zB_y \\
    A_zB_x - A_xB_z \\
    A_xB_y - A_yB_x
\end{pmatrix}
\]

To compute a particular component, cover up that particular row. Then, multiply the component 
of \(\vec{A}\) in the row below (loop to the top if necessary) 
with the diagonal component of \(\vec{B}\) and
subtract it by the product of the component of \(\vec{A}\) two rows below, and the
diagonal component of \(\vec{B}\). For example, for the y-component,

\[
\begin{pmatrix}
    A_x \\
    - \\
    A_z
\end{pmatrix} \cp
%
\begin{pmatrix}
    B_x \\
    - \\
    B_z
\end{pmatrix}
%
\]

And the term simply is \(A_zB_x - A_xb_z\) by looping through the top.

If \(\vec{A} \cp \vec{B} = 0\), then using the component form, 

\begin{align*}
        A_yB_z - A_zB_y &= 0\\
        A_zB_x - A_xB_z &= 0\\
        A_xB_y - A_yB_x &= 0
\end{align*}

Solving for them leads to,

\begin{equation*}
    \frac{A_x}{B_x} = \frac{A_y}{B_y} = \frac{A_z}{B_z}
\end{equation*}

\section{Some geometric results regarding vectors}

\subsection{Lami's Theorem}

Consider three vectors, \(\vec{A}_1, \vec{A}_2, \vec{A}_3\) such that 
\(\vec{A}_1 + \vec{A}_2 + \vec{A}_3 = 0\). If the angles \(\theta_1\), \(\theta_2\)
and \(\theta_3\) are chosen according to \cref{fig: lami}, then,

\begin{theorem}
    [Lami's Theorem]
    \begin{equation*}
        \frac{A_1}{\theta_1} = \frac{A_2}{\theta_2} = \frac{A_3}{\theta_3}
    \end{equation*}
\end{theorem}

\begin{marginfigure}
    \centering
    \scalebox{1.5}{\incfig{lamitheorem}}
    \caption{Lami's Theorem}
    \label{fig: lami}
\end{marginfigure}

The proof follows from arranging the vectors as sides of a triangle and then applying the 
sine law. Note that the angles between the sides would be \(\pi - \theta_i\). Since 
\(\sin(\pi-\theta) = \sin\theta\), we can simply replace them. 

\subsection{Internal and External Segments}

Consider a point \(P\) between the points \(A\) and \(B\) which divides the line segment \(\overline{AB}\) into 
two parts, \(\overline{AP}\) and \(\overline{PB}\) where

\begin{equation*}
    \frac{AP}{PB} = \frac{m}{n}
\end{equation*}

\begin{theorem}
    Given that the position vector of \(A\) is \(\vec{r}_A\) and of \(B\) is 
    \(\vec{r}_B\), the position vector of \(P\) will be,

    \begin{equation}
        \vec{r}_P = \frac{n\vec{r_A} + m\vec{r_B}}{m+n}
    \end{equation}
\end{theorem}

The proof is almost trivial if you know the section theorem. Then we can 
just apply the theorem for all components of \(\vec{r}_{A,B}\) and add them to 
get the desired result. 

The same result if \(P\) lies on the extension of \(\overline{AB}\) and 
\begin{equation*}
    \frac{AP}{BP} = \frac{m}{n}
\end{equation*}

is,

\begin{equation}
    \boxed{\vec{r}_P = \frac{n\vec{r_A} - m\vec{r_B}}{m-n}}
\end{equation}

This can also be derived geometrically by consider two vector triangles. 

Another important result, the component wise proof of which we'll take as granted from 
co-ordinate geometry is of the centroid of a triangle. Given position vectors 
of the vertices \(\vec{r}_A\), \(\vec{r}_B\) and \(\vec{r_C}\), the position 
vector of the centroid of the triangle is,

\begin{equation}
    \boxed{\vec{r}_C = \frac{\vec{r}_A + \vec{r}_B + \vec{r}_C}{3}}
\end{equation}

\begin{marginfigure}
    \centering
    \scalebox{1.6}{\incfig{anglebisect1}}
    \caption{Angle bisector of \(2\theta\).}
\end{marginfigure}

\subsection{Angle Bisectors}

Given two vectors \(\vec{A}\) and \(\vec{B}\), we can find out 
the direction of the angle bisector of the angle between them.

A natural instinct might be to construct something like a parallelogram. Well, 
that doesn't quite work since the resultant does not co-incide with the angle bisector. 

But we can use a parallelogram of \(\unitv{A}\) and \(\unitv{B}\) to construct a rhombus ---
whose diagonal, the resultant of these vectors lies along the angle bisector! Constructing 
and shifting parallelly the external angle bisector, we find it lies along the other diagonal,
the difference of the two unit vectors.

Note that \(\unitv{A} + \unitv{B}\) is not a unit vector, we will need to find the 
unit vector along that direction. 

\begin{marginfigure}
    \centering
    \scalebox{1.7}{\incfig{anglebisect2}}
    \caption{The internal angle bisector lies along \(\unitv{A} + \unitv{B}\) and
    the external lies along \(\unitv{A} - \unitv{B}\).}
\end{marginfigure}

\subsection{Loci}

A set of points that fulfill some particular condition are called a locus. Some loci that will be of 
our use are given.

\subsubsection{Circle}

A point lies on a circle with center at \((h,k)\) if it is at some fixed distance, \(r\), 
the radius from the center. Thus, if the point has co-ordinated \(x, y\) it 
must obey the equation, 

\begin{equation}
    (x-h)^2 + (y-k)^2 = r^2.
\end{equation}

The area of such a circle is \(\pi r^2\) and its perimeter is \(2\pi r\). 

\subsubsection{Sphere}

A natural extension of the circle into \(3\) dimensions is the sphere. 
If the center of the sphere is at \(h, k, f\), then the co-ordinate 
of any point on its surface will be determined by, 

\begin{equation}
    (x-h)^2 + (y-k)^2 + (z-f)^2 = r^2
\end{equation}

For some fixed \(z\), the \(x\) and \(y\) co-ordinates just form 
a circle. Thus, a sphere can be thought of being made up these circles. The idea 
is useful in differential analysis.

\subsubsection{Ellipse}

\begin{marginfigure}
    \centering
    \scalebox{2}{\incfig{ellipse}}
    \caption{Ellipse by Ag2gaeh CC BY-SA 4.0, \url{https://commons.wikimedia.org/w/index.php?curid=57497218}}
    \label{fig: ellipse}
\end{marginfigure}


A ellipse is a weirder shape. It is a sort of stretched circle, and the 
stretching is non-uniform across the two dimensions. In \cref{fig: ellipse}, the 
length of the semi-minor axis is commonly denoted as \(b\) and that of the semi major 
axis is \(a\). Then any point on the ellipse must have co-ordinates \((x,y)\) such that,

\begin{equation}
    \frac{(x-h)^2}{a^2} + \frac{(y-k)^2}{b^2} = 1
\end{equation}

The distance of the foci from the center is its linear eccentricity, \(c = \sqrt{a^2 - b^2}\). 
Overall, the ellipse and other conic sections are characterized by their eccentricity, 

\begin{equation*}
    e = \sqrt{1 - \frac{b^2}{a^2}}
\end{equation*}

The eccentricity of the ellipse ranges from \(0\) which is just a circle, to \(1\) where 
it becomes a parabola. The area of the ellipse is \(\pi ab\). However, its 
perimeter only has a solution through an infinite series integral, so we'll leave it 
out for now.

An ellipse can extended to three dimensions to form a shape called ellipsoid. Its locus is given by,

\begin{equation}
    \frac{(x-h)^2}{a^2} + \frac{(y-k)^2}{b^2} + \frac{(z-f)^2}{c^2} = 1
\end{equation}

Its volume is given by \(\dfrac{4}{3}\pi abc\).

\section{Radius of Curvature}

The \vocab{radius of curvature} is the radius of the circle which best approximates 
a curve at any given point. The talk of a circle at any given point can be non-rigorously 
understood as the circle containing the arc from \((x, f(x))\) to \((x + \dd{x}, f(x + \dd{x}))\). 

We can create a formula of the radius of curvature by first noting that the 
radius at the point, \(R_c\) will be, 

\begin{equation*}
    R_c = \dv{l}{\theta}
\end{equation*}

Where \(\dd{l} = \sqrt{\dd{x}^2 + \dd{y}^2} = \sqrt{1 + (f'(x))^2} \dd{x}\) is uniquely described by that point. 

Now we just need to figure out \(\dd{\theta}\). We can do so by first noting that 
if the curve is the graph of some differentiable function, \(f\), then,

\begin{equation*}
    \tan \theta = f'(x)
\end{equation*}

\begin{marginfigure}
    \scalebox{3}{\incfig{roc}}
    \caption{Circles formed by the radius of curvature for several points. The circles
    formed by positive ones are in red while the ones by the negative ones are in teal. 
    The blue circle has an infinite radius.}
    \label{fig: roc}
\end{marginfigure}

We can differentiate this again with respect to \(x\) to get,

\begin{equation*}
    \sec^2\theta \dv{\theta}{x} = f''(x)
\end{equation*}


Using the identity \(1 + \tan^2\theta = \sec^2 \theta\) and \(\tan^2 \theta = f'(x)\),

\begin{equation*}
    (1 + (f'(x))^2) \dv{\theta}{x} = f''(x)
\end{equation*}

And, 

\begin{equation*}
    \dd{\theta} = \frac{f''(x)}{1 + (f'(x)^2)} \dd{x}
\end{equation*}

Substituting both of these, we get, 

\begin{equation}
    \label{eq: roc}
    \boxed{R_c = \frac{[1 + (f'(x))^2]^{3/2}}{f''(x)}}
\end{equation}


The radius of curvature allows us to say much about \(f''(x)\). In \eqref{eq: roc} 
clearly the numerator is positive. Thus, the sign of the radius of curvature is uniquely determined 
by \(f''(x)\). 

What exactly does the negative sign mean? Well it basically tells whether the circle of 
that radius lies above or below our curve. In \cref{fig: roc}, we 
can easily find out where \(f''(x)\) is negative or 
positive depending on where the circle lies. The blue circle of infinite radius 
is at the \vocab{inflection point}, a point where \(f''(x)\) is \(0\). 

An inflection point occurs here since \(f''(x)\) changes signs from negative to positive
and thus, must become \(0\) at some point.

Given something like a position time graph, 
we can easily find where the acceleration is positive by just well, drawing out circles.
Nice.

\begin{example}
    Find the roc of ellipse at its top most and right most points, \((0,b)\) and \((a, 0)\) centered 
    at \((0,0)\)

    \begin{soln}
        We will here figure out the radius at \((0,b)\) and argue by symmetry for \((a,0)\).
        In well actuality, the roc at \((a, 0)\) needs use of parametrized forms and 
        so does the general roc equation.

        However, for \((0,b)\) we may note that \[\eval{\dv{y}{x}}_{(0, b)}=0.\]
        
        And then implicitly derivate the ellipse equation wrt \(x\). We then obtain,
        
        \begin{equation*}
            \eval{\dv[2]{y}{x}}_{(0,b)} = -\frac{b}{a^2} 
        \end{equation*}

        By using \eqref{eq: roc} we get,

        \begin{equation}
            \eval{R_c}_{(0,b)} = \frac{-a^2}{b}
        \end{equation}

        By a bit of an ad-hoc symmetric argument, the roc at \((a, 0)\) is 
        \(b^2/a\).
    \end{soln} 
\end{example}

\marginnote{It's much better evaluated using the parametric equations.}.

